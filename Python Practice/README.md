# Python study for Deep Learning

## Numpy Study
[1. Objects and ndarrays](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Objects%20and%20ndarrays.ipynb)<br/>
[2. Making ndarrays](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Making%20ndarrays.ipynb)<br/>
[3. Meta data of ndarrays](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Meta-data%20of%20ndarrays.ipynb)<br/>
[4. Changing ndarrays](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Changing%20ndarrays.ipynb)<br/>
[5. Element-wise Operation and Broadcasting](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Element-wise%20Operations%20and%20Broadcasting.ipynb)<br/>
[6. Indexing and Slicing ndarrays](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Indexing%20and%20Slicing%20ndarrays.ipynb)<br/>
[7. axis and keepdims Arguments](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/axis%20and%20keepdims%20Arguments.ipynb)<br/>
[8. Sum, Prod, Diff and Statistics](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Sum%2C%20Prod%2C%20Diff%20and%20Statistics.ipynb)<br/>
[9. Rounding and Sorting](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Rounding%20and%20Sorting.ipynb)<br/>
[10. Mathematical Functions](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Mathematical%20Functions.ipynb)<br/>
[11. Matrix Operation](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Matrix%20Operation.ipynb)<br/>
[12. Dimension Manipulation](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Dimensionality%20Manipulations.ipynb)<br/>
[13. Merging ndarrays](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Merging%20ndarrays.ipynb)<br/>
[14. Repeating ndarrays](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Repeating%20ndarrays.ipynb)<br/>
[15. Tricks for Fully-connected Operations](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Tricks%20for%20Fully-connected%20Operationsipynb)<br/>
[16. Exercise1_Score Calculation](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Exercise%201_Score%20Calculation.ipynb)<br/>
[17. Exercise2_Calculation](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Exercise%202_Calculation%20of%20Vectors.ipynb)<br/>
[18. Exercise3_Random Shuffling](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Numpy%20Study/Exercise%203_Random%20Shuffling.ipynb)<br/>

## Forward Propagation of Neural Networks
[1. Affine Functions](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/1-1_Affine%20Functions.ipynb)<br/>
[2. Artificial Neurons](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/1-2_Artificial%20Neurons.ipynb)<br/>
[3. Dense Layers](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/2-1_Dense%20Layers.ipynb)<br/>
[4. Cascaded Dense Layers](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/2-2_Cascaded%20Dense%20Layer.ipynb)<br/>
[5. Model Implement with Dense Layers](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/2-3_Model%20Implementation%20with%20Dense%20Layers.ipynb)<br/>
[6. Binary Classifiers ](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/3-1_Binary%20Classifiers.ipynb)<br/>
[7. Multi-class Classifiers](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/3-2_Multi-class%20Classifiers.ipynb)<br/>
[8. Toy Datasets for Regression and Binary Classification](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/4-1_Toy%20Datasets%20for%20Regression%20and%20Binary%20Classification.ipynb)<br/>
[9. MSE and BCE](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/4-2_MSE%20and%20BCE.ipynb)<br/>
[10. SCCE and CCE](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/4-3_SCCE%20and%20CCE.ipynb)<br/>
[11. Conv Layers](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/5-1_Conv2D%20Layers.ipynb)<br/>
[12. Conv2D with Filters](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/5-2_Conv2D%20with%20Filters.ipynb)<br/>
[13. Model Implementation with Conv2D Layers](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/5-3_Model%20Implementation%20with%20Conv2D%20Layers.ipynb)<br/>
[14. Max and Average Pooling Layers](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/6-1_Max%20and%20Average%20Pooling%20Layers.ipynb)<br/>
[15. Padding and Strides](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/6-2_Padding%20and%20Strides.ipynb)<br/>
[16. Shapes in CNNs](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/7-1_%20Shapes%20in%20CNN.ipynb)<br/>
[17. CNN Implementation](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/7-2_%20CNN%20Implementation.ipynb)<br/>
[18. LeNet Implementation](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Forward%20Propagation%20of%20Neural%20Networks/7-3_LeNet%20Implementation.ipynb)<br/>

## Backpropagation and Jacobian Matrices
[1. Gradient Based Learning Practice](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Backpropagation%20and%20Jacobian%20Matrices/1_Gradient%20Based%20Learning%20Practice.ipynb)<br/>
[2. Linear Regression 구현](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Backpropagation%20and%20Jacobian%20Matrices/2_Linear%20Regression%20%EA%B5%AC%ED%98%84.ipynb)<br/>
[3. Sigmoid 함수의 변수](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Backpropagation%20and%20Jacobian%20Matrices/3_Sigmoid%20%ED%95%A8%EC%88%98%EC%9D%98%20%EB%B3%80%EC%88%98.ipynb)<br/>
[4. Logistic Regression 구현](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Backpropagation%20and%20Jacobian%20Matrices/4_Logistic%20Regression%20%EA%B5%AC%ED%98%84.ipynb)<br/>
[5. Linear Regression 구현 (Minibatch)](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Backpropagation%20and%20Jacobian%20Matrices/5_Linear%20Regression%20%EA%B5%AC%ED%98%84%20(Minibatch).ipynb)<br/>
[6. Logistic Regression 구현 (Minibatch)](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Backpropagation%20and%20Jacobian%20Matrices/6_Logistic%20Regression%20%EA%B5%AC%ED%98%84%20(Minibatch).ipynb)<br/>
[7. Multi-layer perceptron 구현](https://github.com/kimbyeolhee/TIL/blob/main/Python%20Practice/Backpropagation%20and%20Jacobian%20Matrices/7_Multi%20Layer%20Perceptron%20Implementation.ipynb)
