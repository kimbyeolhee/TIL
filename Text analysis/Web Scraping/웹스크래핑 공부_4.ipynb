{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a27a315",
   "metadata": {},
   "source": [
    "# 웹 스크래핑을 이용하여 나만의 비서 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1094b95",
   "metadata": {},
   "source": [
    "1. 네이버에서 오늘 일산의 날씨 정보를 가져옴\n",
    "\n",
    "2. 헤드라인 뉴스 3건\n",
    "\n",
    "3. IT 뉴스 3건\n",
    "\n",
    "4. 해커스 어학원 홈페이지에서 오늘의 영어 회화 지문을 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334c36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598795c9",
   "metadata": {},
   "source": [
    "### 1. 네이버에서 오늘 일산의 날씨 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "006e4494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[오늘의 날씨]\n",
      "구름많음, 어제보다 3˚ 높아요\n",
      "현재 20℃  (최저 18˚ / 최고 28˚)\n",
      "오전 강수확률 30% / 오후 강수확률 10%\n",
      "\n",
      "미세먼지 12㎍/㎥좋음\n",
      "초미세먼지 9㎍/㎥좋음\n",
      "\n",
      "[헤드라인 뉴스]\n",
      "1. 이상반응 1000만원 지원, 심근염·심낭염·길랭-바레 등 확대\n",
      "  (링크) : https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=102&oid=018&aid=0005032933\n",
      "2. 플랫폼에서 대출·펀드 손쉽게 구매…결국은 소비자 피해로 돌아온다\n",
      "  (링크) : https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=101&oid=028&aid=0002559928\n",
      "3. \"남성 일색 내각 반대\" 아프간 女시위대에 채찍…말 뿐인 '탈레반 2.0'\n",
      "  (링크) : https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=104&oid=421&aid=0005593446\n",
      "\n",
      "[IT 뉴스]\n",
      "1. KT의 지니뮤직, '밀리의 서재' 품는다\n",
      "  (링크) : https://news.naver.com/main/read.naver?mode=LS2D&mid=shm&sid1=105&sid2=230&oid=015&aid=0004603152\n",
      "2. [르포] 수소모빌리티+쇼 개막 이틀째.. 각 기업들 \"수소벨류체인 구축에 사활 건다\"\n",
      "  (링크) : https://news.naver.com/main/read.naver?mode=LS2D&mid=shm&sid1=105&sid2=230&oid=092&aid=0002233305\n",
      "3. SK이노베이션, 에코프로 그룹과 고성능 배터리 시장 선점 위해 ’맞손’\n",
      "  (링크) : https://news.naver.com/main/read.naver?mode=LS2D&mid=shm&sid1=105&sid2=230&oid=092&aid=0002233304\n",
      "\n",
      "[오늘의 영어 회화]\n",
      "영어 지문\n",
      "Danielle : Is there anything you want me to print out before you go to the conference room, Rob?\n",
      "Rob : Yes. I need that revised budget before I meet with Mrs. Rodriguez.\n",
      "Danielle : I believe I placed several copies on your desk.\n",
      "Rob : Oh, you did? Let me go over and check.\n",
      "--------------------------------------------------\n",
      "한글 지문\n",
      "Danielle : 회의실에 들어가시기 전에 출력 부탁하실 것 없으세요, Rob?\n",
      "Rob : 있어요. Mrs. Rodriguez 를 만나기 전에 수정된 예산안이 필요해요.\n",
      "Danielle : 제가 몇 부를 당신 책상 위에 올려둔 것 같은데요.\n",
      "Rob : 오, 그랬어요? 제가 가서 확인해보죠.\n"
     ]
    }
   ],
   "source": [
    "def create_soup(url):\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, \"lxml\")\n",
    "    return soup\n",
    "\n",
    "def print_news(index, title, link):\n",
    "    print(\"{}. {}\".format(index+1, title))\n",
    "    print(\"  (링크) : {}\".format(link))\n",
    "\n",
    "    \n",
    "def scrape_weather():\n",
    "    print(\"[오늘의 날씨]\")\n",
    "    url = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=%EC%9D%BC%EC%82%B0+%EB%82%A0%EC%94%A8\"\n",
    "    soup = create_soup(url)\n",
    "    \n",
    "    # 현재온도 최고/최저 온도\n",
    "    cast = soup.find(\"p\", attrs={\"class\":\"cast_txt\"}).get_text()\n",
    "    curr_temp = soup.find(\"p\", attrs={\"class\":\"info_temperature\"}).get_text().replace(\"도씨\",\"\")\n",
    "    min_temp = soup.find(\"span\", attrs={\"class\":\"min\"}).get_text()\n",
    "    max_temp = soup.find(\"span\", attrs={\"class\":\"max\"}).get_text()\n",
    "    \n",
    "    # 오전 오후 강수 확률\n",
    "    morning_rain_rate = soup.find(\"span\", attrs={\"class\":\"point_time morning\"}).get_text().strip()\n",
    "    afternoon_rain_rate = soup.find(\"span\", attrs={\"class\":\"point_time afternoon\"}).get_text().strip()\n",
    "    \n",
    "    # 미세먼지\n",
    "    dust = soup.find(\"dl\", attrs={\"class\":\"indicator\"})\n",
    "    pm10 = dust.find_all(\"dd\")[0].get_text()\n",
    "    pm25 = dust.find_all(\"dd\")[1].get_text()\n",
    "    \n",
    "    \n",
    "    # 출력\n",
    "    print(cast)\n",
    "    print(\"현재 {} (최저 {} / 최고 {})\".format(curr_temp, min_temp, max_temp))\n",
    "    print(\"오전 {} / 오후 {}\".format(morning_rain_rate, afternoon_rain_rate))\n",
    "    print()\n",
    "    print(\"미세먼지 {}\".format(pm10))\n",
    "    print(\"초미세먼지 {}\".format(pm25))\n",
    "    print()\n",
    "\n",
    "    \n",
    "def scarpe_headline_news():\n",
    "    print(\"[헤드라인 뉴스]\")\n",
    "    url = \"https://news.naver.com\"\n",
    "    soup = create_soup(url)\n",
    "    \n",
    "    news_list = soup.find(\"ul\", attrs={\"class\":\"hdline_article_list\"}).find_all(\"li\", limit=3) # 3개까지만 출력\n",
    "    for index, news in enumerate(news_list):\n",
    "        title = news.find(\"a\").get_text().strip()\n",
    "        link = url + news.find(\"a\")['href']\n",
    "        \n",
    "        print_news(index, title, link)\n",
    "    print()\n",
    "    \n",
    "def scrape_it_news():\n",
    "    print(\"[IT 뉴스]\")\n",
    "    url = \"https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=105&sid2=230\"\n",
    "    soup = create_soup(url)\n",
    "    \n",
    "    news_list = soup.find(\"ul\", attrs={\"class\":\"type06_headline\"}).find_all(\"li\", limit=3)\n",
    "    for index, news in enumerate(news_list):\n",
    "        a_idx = 0\n",
    "        img = news.find(\"img\")\n",
    "        if img:\n",
    "            a_idx = 1  # img 태그가 있으면 1번째 img 태그의 정보를 사용\n",
    "        \n",
    "        a_tag = news.find_all(\"a\")[a_idx]\n",
    "        title = a_tag.get_text().strip()\n",
    "        link = a_tag[\"href\"]\n",
    "        \n",
    "        print_news(index, title, link)\n",
    "    print()\n",
    "\n",
    "def scrape_english():\n",
    "    print(\"[오늘의 영어 회화]\")\n",
    "    url = \"https://www.hackers.co.kr/?c=s_eng/eng_contents/I_others_english\"\n",
    "    soup = create_soup(url)\n",
    "    \n",
    "    sentences = soup.find_all(\"div\", attrs={\"id\":re.compile(\"^conv_kor_t\")})\n",
    "    print(\"영어 지문\")\n",
    "    for sentence in sentences[len(sentences)//2:]:# 8문장이 있다고 가정할 때, index 기준 4~7까지 잘라서 기져옴\n",
    "        print(sentence.get_text().strip())\n",
    "    print(\"-\"*50)    \n",
    "    print(\"한글 지문\")\n",
    "    for sentence in sentences[:len(sentences)//2]:\n",
    "        print(sentence.get_text().strip())\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    scrape_weather()\n",
    "    scarpe_headline_news()\n",
    "    scrape_it_news()\n",
    "    scrape_english()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
